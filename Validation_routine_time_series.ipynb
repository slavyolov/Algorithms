{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validation_routine_time_series.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAD8TlnYvIDLtN4lSz3xuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slavyolov/Algorithms/blob/main/Validation_routine_time_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Issues with labeling in TSAD (Time Series Anomaly Detection)**\n",
        "\n",
        "- https://data-mining.philippe-fournier-viger.com/serious-issues-with-time-series-anomaly-detection-research/\n",
        "  - https://arxiv.org/mwg-internal/de5fs23hu73ds/progress?id=SdqAjHGJel9YSr4Apo7gZuA1vS0rdtm6XNqKFa2xbmQ,\n",
        "\n",
        "- **Eamon Keogh**\n",
        "  - https://kdd-milets.github.io/milets2021/slides/Irrational%20Exuberance_Eammon_Keogh.pdf\n",
        "    - https://www.youtube.com/watch?v=Vg1p3DouX8w\n",
        "  - Benchmark - anomaly detection : www.cs.ucr.edu/~eamonn/time_series_data_2018/\n",
        "    - dataset information | https://www.cs.ucr.edu/~eamonn/time_series_data_2018/BriefingDocument2018.pdf\n",
        "\n",
        "  - **Summary :**\n",
        "    - **perfect ground truth labels are impossible**\n",
        "    - Benchmark datasets (not the one listed above - as mentioned by the author):\n",
        "      - At least 90% of the benchmark datasets can be solved with very simple\n",
        "methods dating back decades, or with “one-liners”.\n",
        "      - At least 90% of the benchmark datasets can be solved without needing\n",
        "to even look at the training data!\n",
        "      - This should be worrisome. In what sense do we need machine learning,\n",
        "or deep learning, when it is not clear we need to learn from the training\n",
        "data in any way?\n",
        "for anomaly detection!\n",
        "    - **Anomaly detection is meant to be “expect the unexpected”**. If you can concretely define in a single English sentence what you expect\n",
        "to find in advance, is that really anomaly detection? - No!\n",
        "    - **Anomaly discovery methods typically have two parts**\n",
        "      - (a) finding the region(s) most likely to be an anomaly\n",
        "      - (b) using some “threshold” to predict if it really is an anomaly.\n",
        "\n",
        "        - We may be wrong, but we see ‘a’ as being the hardest part of the challenge. In any case, ‘b’\n",
        "can sometimes depend on some out-of-band information.\n",
        "        -  We can completely remove the ‘b’ question by having exactly one anomaly per dataset, and\n",
        "telling people there is exactly one anomaly per dataset.\n",
        "    - **Scoring Function Design Principles**\n",
        "      - Let length of anomaly be L, L = end – begin\n",
        "      - Let the prediction of an algorithm be an integer P\n",
        "      - P is labeled as correct if: min(begin-L , begin-100 ) < P < max(end+L, end+100 )\n",
        "      - Why the ‘100’ case? Some anomalies can be as short as a single point.\n",
        "- Paper - \"Temporal\n",
        "convolutional autoencoder for unsupervised anomaly detection in time series\" "
      ],
      "metadata": {
        "id": "Ga4BvhtaSpz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation - Anomaly Detection**\n",
        "\n",
        "- https://towardsdatascience.com/anomaly-detection-how-to-tell-good-performance-from-bad-b57116d71a10\n",
        "- Sumamry : \n",
        "  - Pick the system with the **lowest possible False Positive rate**. If the False Positive rate is too high, the users will turn off the system since it is more distracting than useful.\n",
        "  - Choose the system with the **lowest possible False Negatives rate**. If the False Negative rate is too high, you will be missing a lot of crucial anomalies and in time you will lose trust in the system.\n",
        "\n",
        "- https://medium.com/balabit-unsupervised/how-to-evaluate-unsupervised-anomaly-detection-for-user-behavior-analytics-88f3d5de2018\n",
        "\n",
        "- Matrix Profile - Eamon Keogh \n",
        "  - https://www.cs.ucr.edu/~eamonn/MatrixProfile.html\n",
        "  - https://towardsdatascience.com/introduction-to-matrix-profiles-5568f3375d90"
      ],
      "metadata": {
        "id": "JQ6VE94f1s4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation routine/Model Monitoring(model management life cycle) in Time series data : \n",
        "\n",
        "## Links\n",
        "- https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
        "- https://towardsdatascience.com/why-is-model-validation-so-darn-important-and-how-is-it-different-from-model-monitoring-61dc7565b0c\n",
        "- https://www.moodysanalytics.com/-/media/article/2019/Effective-Model-Validation.pdf\n",
        "\n",
        "## Model Validation VS Model Monitoring\n",
        "**- The Time period:**\n",
        "  - Model validation is carried out in tandem with the model development process. This is immediately after the model development. It’s a one-time process.\n",
        "  - Model Monitoring comes into effect after a model has gone into the PROD (production) stage. It’s an ongoing process. A specific monitoring frequency is decided for every model and it’s evaluated then to make sure that the model is performing up to the mark and its results are reliable. Also, we check that the population distribution should not be significantly different as compared to the development time period to ensure that the model is still relevant and okay to be used.\n",
        "\n",
        "**- The Purpose:**\n",
        "  - The purpose of model validation is to check the accuracy and performance of the model basis on the past data for which we already have actuals.\n",
        "  - Once the model is deployed, the model monitoring processes ensure the relevance of the model by judging the population distribution and also recording back-dated error % comparisons between the model predictions and actuals data as soon as that starts coming in to make sure that the model performance is in the acceptable range.\n",
        "\n",
        "**- The Metrics:**\n",
        "  -  In the model validation stage, we focus majorly on the statistical metrics that can decode the model performance and response for us.\n",
        "  - in the model monitoring stage, we focus both on the statistical as well the business metrics to derive our conclusion of being confident in the relevance and the reliability of a particular model.\n",
        "\n",
        "## Sumamry : \n",
        "- **we must split data up and respect the temporal order (backtesting)**  - the time dimension of observations means that we cannot randomly split them into groups. Instead, we must split data up and respect the temporal order in which values were observed.\n",
        "- Methods : \n",
        "  -  Train-Test split that respect temporal order of observations.\n",
        "    - It is useful when you have a large amount of data so that both training and tests sets are representative of the original problem.\n",
        "  - Multiple Train-Test splits that respect temporal order of observations.\n",
        "    - This will require multiple models to be trained and evaluated, but this additional computational expense will provide a more robust estimate of the expected performance of the chosen method and configuration on unseen data.\n",
        "    - the scikit-learn library provides this capability for us in the TimeSeriesSplit object. \n",
        "    - Using multiple train-test splits will result in more models being trained, and in turn, a more accurate estimate of the performance of the models on unseen data.\n",
        "\n",
        "    - A limitation of the train-test split approach is that the trained models remain fixed as they are evaluated on each evaluation in the test set.\n",
        "  - Walk-Forward Validation where a model may be updated each time step new data is received.\n"
      ],
      "metadata": {
        "id": "CMZWG4qFb4IZ"
      }
    }
  ]
}